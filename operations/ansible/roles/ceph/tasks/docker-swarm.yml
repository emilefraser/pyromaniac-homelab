
- name: download latest cephadm
  get_url:
    url: https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm
    dest: /usr/local/sbin/cephadm
    mode: 0755 

- name: add ceph repo apt-key
  apt_key: 
    url: https://download.ceph.com/keys/release.asc
    state: present 
  
- name: install ceph repo
  shell: cephadm add-repo --release octopus
  
- name: install ceph tools
  apt:
    update_cache: yes
    name: "{{ packages }}"
  vars:
    packages:
    - ceph-common
    - nfs-common

- name: ensure /etc/ceph exists
  file:
    path: /etc/ceph
    state: directory

- name: ensure /var/data exists
  file:
    path: /var/data
    state: directory

- name: add the hosts in the group to /etc/hosts
  lineinfile:
    dest: /etc/hosts
    regexp: '.*{{ item }}$'
    line: "{{ hostvars[item]['ansible_default_ipv4']['address'] }} {{item}}"
    state: present
  when: hostvars[item]['ansible_facts']['default_ipv4'] is defined
  with_items:
    - "{{ play_hosts }}"  

- name: prevent cloud-init from overwriting /etc/hosts
  lineinfile:
    dest: /etc/cloud/cloud.cfg
    regex: "update_etc_hosts"
    state: absent

- name: do initial part of ceph cluster setup
  run_once: true
  delegate_to: "{{ play_hosts[0] }}"
  block:

  - name: have we already been bootstrapped?
    stat:
      path: /etc/ceph/ceph.conf
    register: _ceph_configured

  - name: bootstrap ceph cluster on first host
    command: "cephadm bootstrap --mon-ip {{ hostvars[play_hosts[0]].ansible_default_ipv4.address }}"
    when: not _ceph_configured.stat.exists

  - name: write dashboard password to disk
    copy: 
      dest: /tmp/ceph-dashboard-credentials
      content: "{{ ceph_dashboard_password }}"

  - name: add credentials for dashboard access
    command: "ceph dashboard ac-user-create {{ ceph_dashboard_username }} administrator -i /tmp/ceph-dashboard-credentials"
    when: not _ceph_configured.stat.exists

  - name: remove dashboard password from disk
    file: 
      path: /tmp/ceph-dashboard-credentials
      state: absent

  - name: get SSH public key from first host
    command: cat /etc/ceph/ceph.pub
    register: cephadm_ssh_public_key

  - name: get ceph admin keyring from first host
    command: cat /etc/ceph/ceph.client.admin.keyring
    register: ceph_admin_keyring

  - name: get ceph/ceph.conf from first host
    command: cat /etc/ceph/ceph.conf
    register: ceph_conf

- name: prepare other hosts to join the cluster
  when: inventory_hostname != play_hosts[0]
  block: 

  - name: deploy cephadm ssh key on other hosts
    authorized_key: 
      user: root
      key: "{{ cephadm_ssh_public_key.stdout }}"   

  - name: deploy ceph admin keyring to other hosts
    copy: 
      dest: /etc/ceph/ceph.client.admin.keyring
      content: "{{ ceph_admin_keyring.stdout }}\n"            

  - name: deploy ceph/ceph.conf to other hosts
    copy: 
      dest: /etc/ceph/ceph.conf
      content: "{{ ceph_conf.stdout }}\n"            

- name: expand the cluster to the other hosts and setup file system
  delegate_to: "{{ play_hosts[0] }}"
  run_once: true
  block:

  - name: add remaining hosts to ceph cluster
    command: ceph orch host add {{ inventory_hostname }} {{ ansible_default_ipv4.address }}
    run_once: false

  - name: add all available devices as OSDs
    command: ceph orch apply osd --all-available-devices  

  - name: is our cephfs created yet?
    shell: ceph fs ls | grep data
    register: _cephfs_created
    failed_when: _cephfs_created.rc not in [0,1]

  - name: create cephfs (if it doesn't already exist)
    command: ceph fs volume create data
    when: _cephfs_created.rc == 1

  - name: do we have an mds up yet? (try for 20 min)
    shell: ceph -s | grep mds | grep 'data:1'
    register: _result
    until: _result.failed == false
    retries: 120 # retry X times  
    delay: 10    # pause for X sec b/w each call  

  - name: concatenate the names of all the hosts for the next 2 steps
    set_fact:
      _ceph_mons: "{{ play_hosts | join(',') }}" 

  - name: enable mds on all hosts
    command:
      cmd: "ceph orch apply mds data --placement='{{ _ceph_mons }}'"
    run_once: true
    delegate_to: "{{ play_hosts[0] }}"

- name: add cephfs mount to /etc/fstab, and mount it
  mount:
    src: "{{ _ceph_mons }}:/"
    path: /var/data
    state: mounted
    fstype: ceph
    opts: name=admin,noatime,_netdev
  register: _result        
  until: _result.failed == false
  retries: 60 # retry X times  
  delay: 5     # pause for X sec b/w each call
#   when:
#   - "'ci' not in group_names" # Don't actually mount cephfs for CI hosts (too underpowered)

# # Only for CI, ignore for normal users!
# - name: add ci nfs mount to /etc/fstab, and mount it
#   mount:
#     src: "192.168.39.110:/var/data"
#     path: /var/data
#     state: mounted
#     fstype: nfs
#     opts: hard,intr,noatime,_netdev,local_lock=all
#   when:
#   - "'ci' in group_names" # Don't actually mount cephfs for CI hosts (too underpowered)  